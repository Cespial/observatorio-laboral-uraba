name: Scrape Profundo (Semanal)

on:
  schedule:
    # Run every Sunday at 3 AM Colombia time (UTC-5 = 8 AM UTC)
    - cron: '0 8 * * 0'
  workflow_dispatch:  # Allow manual trigger

jobs:
  scrape-deep:
    runs-on: ubuntu-latest
    timeout-minutes: 360  # 6 horas

    steps:
      - name: Checkout observatorio
        uses: actions/checkout@v4

      - name: Checkout scraper repo
        uses: actions/checkout@v4
        with:
          repository: Cespial/uraba-empleos
          path: uraba_empleos
          token: ${{ secrets.GITHUB_TOKEN }}
        continue-on-error: true

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install scraper dependencies
        run: |
          pip install playwright pandas beautifulsoup4 fake-useragent tabulate unidecode openpyxl pdfplumber
          playwright install chromium
        if: hashFiles('uraba_empleos/requirements.txt') != ''

      - name: Install ETL dependencies
        run: |
          pip install sqlalchemy psycopg2-binary

      - name: Run deep scraper
        run: |
          cd uraba_empleos
          python main.py --deep --sources computrabajo elempleo indeed comfama magneto365 talent jooble
        timeout-minutes: 350
        continue-on-error: true

      - name: Copy SQLite to home for ETL
        run: |
          mkdir -p ~/uraba_empleos
          cp uraba_empleos/empleos_uraba.db ~/uraba_empleos/empleos_uraba.db
        if: hashFiles('uraba_empleos/empleos_uraba.db') != ''

      - name: Sync to Supabase
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
        run: |
          python etl/12_sync_empleo_incremental.py

      - name: Summary
        run: |
          echo "## Scraping Summary (Deep - Weekly)" >> $GITHUB_STEP_SUMMARY
          echo "- Date: $(date -u)" >> $GITHUB_STEP_SUMMARY
          if [ -f uraba_empleos/empleos_uraba.db ]; then
            COUNT=$(sqlite3 uraba_empleos/empleos_uraba.db "SELECT COUNT(*) FROM ofertas" 2>/dev/null || echo "N/A")
            echo "- Total offers in SQLite: $COUNT" >> $GITHUB_STEP_SUMMARY
            echo "### By source:" >> $GITHUB_STEP_SUMMARY
            sqlite3 uraba_empleos/empleos_uraba.db "SELECT fuente, COUNT(*) as total FROM ofertas GROUP BY fuente ORDER BY total DESC" 2>/dev/null | while IFS='|' read fuente total; do
              echo "- $fuente: $total" >> $GITHUB_STEP_SUMMARY
            done
          fi

      - name: Alert on failure
        if: failure()
        uses: actions/github-script@v7
        with:
          script: |
            await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: `[Alert] Deep scraping failed - ${new Date().toISOString().split('T')[0]}`,
              body: `## Scraping Failure Alert\n\n- **Workflow:** ${context.workflow}\n- **Run:** ${context.runId}\n- **Date:** ${new Date().toISOString()}\n\n[View run](${context.serverUrl}/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId})`,
              labels: ['bug', 'scraping']
            })
