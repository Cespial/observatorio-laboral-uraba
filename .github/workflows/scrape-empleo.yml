name: Scrape Diario (Incremental)

on:
  schedule:
    # Run every day at 6 AM and 6 PM Colombia time (UTC-5)
    - cron: '0 11,23 * * *'
  workflow_dispatch:  # Allow manual trigger

jobs:
  scrape-and-sync:
    runs-on: ubuntu-latest
    timeout-minutes: 45

    steps:
      - name: Checkout observatorio
        uses: actions/checkout@v4

      - name: Checkout scraper repo
        uses: actions/checkout@v4
        with:
          repository: Cespial/uraba-empleos
          path: uraba_empleos
          token: ${{ secrets.GITHUB_TOKEN }}
        continue-on-error: true

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install scraper dependencies
        run: |
          pip install playwright pandas beautifulsoup4 fake-useragent tabulate unidecode openpyxl pdfplumber
          playwright install chromium
        if: hashFiles('uraba_empleos/requirements.txt') != ''

      - name: Install ETL dependencies
        run: |
          pip install sqlalchemy psycopg2-binary

      - name: Run scraper (incremental)
        run: |
          cd uraba_empleos
          python main.py --sources computrabajo elempleo indeed comfama linkedin sena comfenalco magneto365 talent jooble
        timeout-minutes: 40
        continue-on-error: true

      - name: Copy SQLite to home for ETL
        run: |
          mkdir -p ~/uraba_empleos
          cp uraba_empleos/empleos_uraba.db ~/uraba_empleos/empleos_uraba.db
        if: hashFiles('uraba_empleos/empleos_uraba.db') != ''

      - name: Sync to Supabase
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
        run: |
          python etl/12_sync_empleo_incremental.py

      - name: Summary
        run: |
          echo "## Scraping Summary (Incremental)" >> $GITHUB_STEP_SUMMARY
          echo "- Date: $(date -u)" >> $GITHUB_STEP_SUMMARY
          if [ -f uraba_empleos/empleos_uraba.db ]; then
            COUNT=$(sqlite3 uraba_empleos/empleos_uraba.db "SELECT COUNT(*) FROM ofertas" 2>/dev/null || echo "N/A")
            echo "- Total offers in SQLite: $COUNT" >> $GITHUB_STEP_SUMMARY
            echo "### By source:" >> $GITHUB_STEP_SUMMARY
            sqlite3 uraba_empleos/empleos_uraba.db "SELECT fuente, COUNT(*) as total FROM ofertas GROUP BY fuente ORDER BY total DESC" 2>/dev/null | while IFS='|' read fuente total; do
              echo "- $fuente: $total" >> $GITHUB_STEP_SUMMARY
            done
          fi

      - name: Alert on failure
        if: failure()
        uses: actions/github-script@v7
        with:
          script: |
            await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: `[Alert] Scraping incremental failed - ${new Date().toISOString().split('T')[0]}`,
              body: `## Scraping Failure Alert\n\n- **Workflow:** ${context.workflow}\n- **Run:** ${context.runId}\n- **Date:** ${new Date().toISOString()}\n\n[View run](${context.serverUrl}/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId})`,
              labels: ['bug', 'scraping']
            })
